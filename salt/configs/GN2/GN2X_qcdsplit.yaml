name: GN2X_qcdsplit

model:
  lrs_config:
    initial: 1e-7
    max: 3e-4
    end: 5e-5
    pct_start: 0.01
    weight_decay: 1e-5

  model:
    class_path: salt.models.SaltModel
    init_args:
      init_nets:
        - input_name: tracks
          dense_config:
            output_size: &embed_dim 256
            hidden_layers: [256]
            activation: &activation ReLU
        - input_name: flow
          dense_config:
            output_size: *embed_dim
            hidden_layers: [256]
            activation: *activation
        - input_name: truth_hadrons
          dense_config:
            output_size: *embed_dim
            hidden_layers: [256]
            activation: *activation

      encoder:
        class_path: salt.models.Transformer
        init_args:
          num_layers: 8
          embed_dim: *embed_dim
          out_dim: &out_dim 128
          attn_type: flash-varlen
          norm: LayerNorm
          dense_kwargs:
            activation: *activation
            dropout: 0.1
            gated: True
          attn_kwargs:
            num_heads: 8
            dropout: 0.1
          num_registers: 8

      pool_net:
        class_path: salt.models.GlobalAttentionPooling
        init_args: { input_size: *out_dim }

      tasks:
        class_path: torch.nn.ModuleList
        init_args:
          modules:
            - class_path: salt.models.ClassificationTask
              init_args:
                name: jets_classification
                input_name: jets
                label: flavour_label
                class_names:
                  ["hbb", "hcc", "top", "qcdbb", "qcdbx", "qcdcx", "qcdxx"]
                loss:
                  class_path: torch.nn.CrossEntropyLoss
                dense_config: &task_dense_config
                  input_size: *out_dim
                  output_size: 7
                  hidden_layers: [128, 64, 32]
                  activation: *activation

            - class_path: salt.models.ClassificationTask
              init_args:
                name: track_origin
                input_name: tracks
                label: ftagTruthOriginLabel
                weight: 0.33
                loss:
                  class_path: torch.nn.CrossEntropyLoss
                dense_config:
                  <<: *task_dense_config
                  output_size: 8
                  context_size: *out_dim

            - class_path: salt.models.VertexingTask
              init_args:
                name: track_vertexing
                input_name: tracks
                label: ftagTruthVertexIndex
                weight: 0.75
                loss:
                  class_path: torch.nn.BCEWithLogitsLoss
                  init_args: { reduction: none }
                dense_config:
                  <<: *task_dense_config
                  input_size: 256
                  output_size: 1
                  context_size: *out_dim

data:
  labeller_config:
    use_labeller: True
    class_names: ["hbb", "hcc", "top", "qcdbb", "qcdbx", "qcdcx", "qcdxx"]
    require_labels: False

  num_inputs:
    tracks: 100
    flow: 100

  variables:
    jets:
      - pt
      - eta
    tracks:
      - d0
      - z0SinTheta
      - dphi
      - deta
      - qOverP
      - IP3D_signed_d0_significance
      - IP3D_signed_z0_significance
      - phiUncertainty
      - thetaUncertainty
      - qOverPUncertainty
      - numberOfPixelHits
      - numberOfSCTHits
      - numberOfInnermostPixelLayerHits
      - numberOfNextToInnermostPixelLayerHits
      - numberOfInnermostPixelLayerSharedHits
      - numberOfInnermostPixelLayerSplitHits
      - numberOfPixelSharedHits
      - numberOfPixelSplitHits
      - numberOfSCTSharedHits
      - leptonID
    flow:
      - flow_pt
      - flow_energy
      - flow_deta
      - flow_dphi
    truth_hadrons:
      - charge
      - pt
      - mass
      - energy
      - eta
      - phi
      - deta
      - dphi
      - dr
      - Lxy
  num_train: 30000000
  num_val: 3000000
  num_test: 3000000

  train_file: /share/lustre/ecritelli/xbb_preprocessing_v2/output/pp_output_train.h5
  val_file: /share/lustre/ecritelli/xbb_preprocessing_v2/output/pp_output_val.h5
  norm_dict: /share/lustre/ecritelli/xbb_preprocessing_v2/output/norm_dict.yaml
  class_dict: /share/lustre/ecritelli/xbb_preprocessing_v2/output/class_dict.yaml

  batch_size: 500
  num_workers: 20

trainer:
  max_epochs: 20
  accelerator: gpu
  devices: 3
  precision: bf16 #bf16-mixed
