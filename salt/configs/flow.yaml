name: flow

data:
  train_file: /share/rcifdata/svanstroud/data/flow/preprocessed/PFlow-hybrid-resampled_scaled_shuffled.h5
  val_file: /share/rcifdata/svanstroud/data/flow/preprocessed/PFlow-hybrid-validation-resampled_scaled_shuffled.h5
  norm_dict: /share/rcifdata/svanstroud/data/flow/scale_dict.json
  #move_files_temp: /dev/shm/svanstro/salt/
  variables:
    jets:
      - pt_btagJes
      - eta_btagJes
    tracks:
      - d0
      - z0SinTheta
      - dphi
    flow:
      - deta
      - dphi

model:
  model:
    class_path: salt.models.JetTagger
    init_args:
      init_nets:
        - input_name: tracks
          dense_config: &init
            input_size: 5
            output_size: &embed_dim 128
            hidden_layers: [256]
            activation: &activation SiLU
            norm_layer: &norm_layer LayerNorm
        - input_name: flow
          dense_config:
            <<: *init
            input_size: 4

      encoder:
        class_path: salt.models.TransformerEncoder
        init_args:
          embed_dim: *embed_dim
          num_layers: 6
          mha_config:
            num_heads: 8
            attention:
              class_path: salt.models.ScaledDotProductAttention
              init_args:
                dropout: 0.0
            out_proj: False
          dense_config:
            norm_layer: *norm_layer
            activation: *activation
            hidden_layers: [256]
            dropout: &dropout 0.1

      pool_net:
        class_path: salt.models.GlobalAttentionPooling
        init_args: { input_size: *embed_dim }

      tasks:
        class_path: torch.nn.ModuleList
        init_args:
          modules:
            - class_path: salt.models.ClassificationTask
              init_args:
                name: jet_classification
                input_name: jets
                label: flavour_label
                loss: torch.nn.CrossEntropyLoss
                dense_config:
                  input_size: *embed_dim
                  output_size: 3
                  hidden_layers: [128, 64, 32]
                  activation: *activation
                  norm_layer: *norm_layer
                  dropout: *dropout
